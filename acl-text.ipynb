{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sichi\\pycharmprojects\\nlpproiect\\venv\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\users\\sichi\\pycharmprojects\\nlpproiect\\venv\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "c:\\users\\sichi\\pycharmprojects\\nlpproiect\\venv\\lib\\site-packages\\numpy\\.libs\\libopenblas.QVLO2T66WEPI7JZ63PS3HMOHFEY472BC.gfortran-win_amd64.dll\n",
      "c:\\users\\sichi\\pycharmprojects\\nlpproiect\\venv\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import json\n",
    "import gc\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "import re\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest,chi2\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from transformers import AutoTokenizer, AutoModel ,BertForTokenClassification,TFBertForSequenceClassification\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from thundersvm import SVR\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# path_to_json = 'judgments_ECtHR/judgments_ECtHR/'\n",
    "# json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Dumping jsons to csv\n",
    "columns = ['itemid', 'docname', 'appno','conclusion','importance','originatingbody','typedescription','kpdate','extractedappno','doctypebranch','respondent','article','url','text','law']\n",
    "# df = pd.DataFrame()\n",
    "# for index, js in enumerate(json_files):\n",
    "#     with open(os.path.join(path_to_json, js)) as json_file:\n",
    "#         json_text = json.load(json_file)\n",
    "#         for key,val in json_text[0].items():\n",
    "#             json_text[0][key] = str(val)\n",
    "#         df = df.append(json_text[0],ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# #GET NAMED ENTITIES AND DUMP THEM\n",
    "# import spacy\n",
    "# spacy_nlp.disable_pipes('tagger')\n",
    "# spacy_nlp = spacy.load('C:\\\\Users\\\\sichi\\\\PycharmProjects\\\\NLPproiect\\\\venv\\\\Lib\\\\site-packages\\\\en_core_web_sm\\\\en_core_web_sm-2.3.1')\n",
    "# tokens = []\n",
    "# spacy_nlp.max_length = 679359709\n",
    "# texts = df['text'].apply(lambda x: ' '.join([word.text for word in spacy_nlp.tokenizer(x)]))\n",
    "# for text in texts:\n",
    "#     tokens.append(set([ent.text for ent in spacy_nlp(text) if ent.ent_type_ == 'PERSON']))\n",
    "# print(tokens)\n",
    "# s = set()\n",
    "# for token in tokens:\n",
    "#     s = s.union(token)\n",
    "# print(s)\n",
    "\n",
    "# with open(\"ners.txt\", \"wb\") as fp:\n",
    "#     pickle.dump(set(s), fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spacy_nlp = spacy.load('C:\\\\Users\\\\sichi\\\\PycharmProjects\\\\NLPproiect\\\\venv\\\\Lib\\\\site-packages\\\\en_core_web_sm\\\\en_core_web_sm-2.3.1')\n",
    "months = [\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\"september\",\"october\",\"november\",\"december\"]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df = pd.read_csv('input.csv',usecols=[\"kpdate\", \"appno\", \"extractedappno\",\"text\",\"law\"], dtype={col:str for col in df.columns})\n",
    "years = [str(i) for i in range(1960,2021)]\n",
    "with open(\"ners2.txt\", \"rb\") as fp:\n",
    "    ners = set(pickle.load(fp))\n",
    "\n",
    "def clean_numbers(row,col):\n",
    "   # year = row['kpdate'].split('-')[0]\n",
    "    article = row['appno']\n",
    "    if article is None:\n",
    "        article = row['extractedappno']\n",
    "    for year in years:\n",
    "        row[col] = row[col].replace(year,'YEAR')\n",
    "    for x in article.split(';'):\n",
    "        row[col] = row[col].replace(x,'APPNUMBER')\n",
    "    if(re.search(\"^\\s*[0-9]+\\.\",str(row['law']))):\n",
    "        app_n = str(row['law']).split('.')[0] + '.'\n",
    "        row[col] = row[col].replace(app_n,'NUMBER')\n",
    "    return row[col]\n",
    "        \n",
    "df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "for index, row in df.iterrows():\n",
    "    df.at[index, 'text'] = clean_numbers(row,'text')\n",
    "df['text'] = df['text'].apply(lambda x: spacy_nlp.tokenizer(x))\n",
    "df['text'] = df['text'].apply(lambda sentence: ' '.join([token.lemma_ for token in sentence if token.is_stop == False and token.is_punct == False and token.text not in ners and token.text not in years]))\n",
    "#and len(token.text) > 2\n",
    "\n",
    "# df['law'] = df['law'].apply(lambda x: str(x).lower())\n",
    "# for index, row in df.iterrows():\n",
    "#     df.at[index, 'law'] = clean_numbers(row,'law')\n",
    "# df['law'] = df['law'].apply(lambda x: spacy_nlp.tokenizer(x))\n",
    "# df['law'] = df['law'].apply(lambda sentence: ' '.join([token.lemma_ for token in sentence if token.is_stop == False and token.is_punct == False]))\n",
    "\n",
    "df['kpdate'] = df['kpdate'].apply(lambda date: float(int(date.split('-')[0]) + float((int(date.split('-')[1]) - 1)/12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_special_mae(predicted):\n",
    "    s = 0\n",
    "    for i,y in enumerate(predicted):\n",
    "        if( (y_test.values[i] < 2000 and predicted[i] > 2000 ) or (y_test.values[i] > 2000 and predicted[i] < 2000 )):\n",
    "            print(\"DAMN:\" + str(y_test.values[i]) + ' ' + str(predicted[i]))\n",
    "        if(y_test.values[i] < 2000 and predicted[i] < 2000):\n",
    "            diff = 0\n",
    "        else:\n",
    "            diff = abs(y_test.values[i] - predicted[i])\n",
    "            s = s + diff\n",
    "\n",
    "    print(str(s/len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['kpdate'], test_size=0.1, random_state=0)\n",
    "tf_vectorizer=TfidfVectorizer(ngram_range=(1,2),encoding='utf-8',sublinear_tf=True) #min_df=0 , #binary maybe sublinear_tf=true\n",
    "X_train=tf_vectorizer.fit_transform(X_train)\n",
    "X_test=tf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102 1.8543067647875409\n",
      "102 1.8901811037647014\n",
      "102 1.928351069981783\n",
      "102 1.968560433333985\n",
      "102 2.0095345255195913\n",
      "102 2.0522464681770747\n",
      "102 2.0959153018667807\n",
      "102 2.1403776690778082\n",
      "102 2.1857679458458166\n",
      "102 2.2319767392774783\n"
     ]
    }
   ],
   "source": [
    "svr = LinearSVR(C=100) #13 for 1-gram\n",
    "svr.fit(X_train,y_train)\n",
    "predicted = svr.predict(X_test)\n",
    "\n",
    "mean = mean_absolute_error(y_test, predicted)\n",
    "print(str(mean)) #1.8510149964071645"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['kpdate'], test_size=0.1, random_state=0)\n",
    "tf_vectorizer=TfidfVectorizer(ngram_range=(1,1),encoding='utf-8',sublinear_tf=True) #min_df=0 , #binary maybe sublinear_tf=true\n",
    "X_train=tf_vectorizer.fit_transform(X_train)\n",
    "X_test=tf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "svr = LinearSVR(C=99) #13 for 1-gram\n",
    "svr.fit(X_train,y_train)\n",
    "predicted = svr.predict(X_test)\n",
    "\n",
    "mean = mean_absolute_error(y_test, predicted)\n",
    "print(str(mean)) #1.5862547589534721"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['kpdate'], test_size=0.1, random_state=0)\n",
    "tf_vectorizer=TfidfVectorizer(ngram_range=(1,1),encoding='utf-8',sublinear_tf=True) #min_df=0 , #binary maybe sublinear_tf=true\n",
    "X_train=tf_vectorizer.fit_transform(X_train)\n",
    "X_test=tf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "clf = KernelRidge(kernel='rbf',alpha=0.005,gamma = 0.05)\n",
    "clf.fit(X_train,y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "mean = mean_absolute_error(y_test, predicted)\n",
    "print(gamm,mean) #1.311677646656628"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['kpdate'], test_size=0.1, random_state=0)\n",
    "tf_vectorizer=TfidfVectorizer(ngram_range=(1,2),encoding='utf-8',sublinear_tf=True) #min_df=0 , #binary maybe sublinear_tf=true\n",
    "X_train=tf_vectorizer.fit_transform(X_train)\n",
    "X_test=tf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "clf = KernelRidge(kernel='rbf',alpha=0.005,gamma = 0.02)\n",
    "clf.fit(X_train,y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "mean = mean_absolute_error(y_test, predicted)\n",
    "print(gamm,mean) #1.109751932425466"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(abs(svr.coef_[0]), tf_vectorizer.get_feature_names()).nlargest(50).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_coefficients(classifier, feature_names, top_features=100):\n",
    "    coef = classifier.coef_.ravel()\n",
    "    top_positive_coefficients = np.argsort(coef)[-top_features:]\n",
    "    top_negative_coefficients = np.argsort(coef)[:top_features]\n",
    "    top_coefficients = np.hstack([top_negative_coefficients, top_positive_coefficients])\n",
    "    # create plot\n",
    "    plt.figure(figsize=(100, 10))\n",
    "    feature_names = np.array(feature_names)\n",
    "    plt.bar(np.arange(top_features), coef[top_negative_coefficients], color=['red'] * (top_features + 1))\n",
    "    plt.xticks(np.arange(1, 1 + top_features), feature_names[top_negative_coefficients], rotation=60, ha='right')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(100, 10))\n",
    "    plt.bar(np.arange(top_features), coef[top_positive_coefficients], color=['blue'] * ( top_features + 1))\n",
    "    plt.xticks(np.arange(1, 1 + top_features), feature_names[top_positive_coefficients], rotation=60, ha='right')\n",
    "    plt.show()\n",
    "plot_coefficients(svr, tf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "feature_names = tf_vectorizer.get_feature_names() \n",
    "coefs_with_fns = sorted(zip(svr.coef_, feature_names)) \n",
    "df=pd.DataFrame(coefs_with_fns)\n",
    "df.columns='coefficient','word'\n",
    "df = df.sort_values(by='coefficient',ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"best_features_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "tf_vectorizer=TfidfVectorizer(ngram_range=(1,2),encoding='utf-8',sublinear_tf=True,min_df=2,max_df=0.9) #min_df=0 , #binary maybe sublinear_tf=true\n",
    "svd = TruncatedSVD(n_components=600)\n",
    "X=svd.fit_transform(tf_vectorizer.fit_transform(text))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, dates, test_size=0.1, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "clf = SVR(kernel='rbf',C=40,gamma = 3)\n",
    "clf.fit(X_train,y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "mean = mean_absolute_error(y_test, predicted)\n",
    "print(str(mean)) #0.8359267099173915"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "#from thundersvm import SVR\n",
    "import numpy as np\n",
    "params = {'kernel': ['rbf'],\n",
    "          'C': np.logspace(1, 50, 17),\n",
    "          'gamma': np.logspace(1, 15, 17)}\n",
    "# for c in np.linspace(10,40,30):\n",
    "#     for gam in np.linspace(2,12,10):\n",
    "#clf = SVR(kernel='rbf',C=c,gamma = gam)\n",
    "\n",
    "grid = RandomizedSearchCV(estimator=SVR(), param_distributions=params,verbose=2,scoring='neg_mean_absolute_error')\n",
    "grid.fit(X, df['kpdate'])\n",
    "print(\"Best parameter values:\", grid.best_params_)\n",
    "print(\"CV Score with best parameter values:\", grid.best_score_)\n",
    "#clf.fit(X_train,y_train)\n",
    "#Best parameter values: {'kernel': 'rbf', 'gamma': 10.0, 'C': 2.7384196342643614e+22}\n",
    "\n",
    "# predicted = clf.predict(X_test)\n",
    "# mean = mean_absolute_error(y_test, predicted)\n",
    "# print(str(c),str(gam),str(mean))\n",
    "#calculate_special_mae(predicted) #0.7128941956283799"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "                  'kernel':['rbf'],\n",
    "                  'C':np.linspace(5,30,10),\n",
    "                  'gamma':np.linspace(1,10,5),\n",
    "                  'epsilon':np.linspace(0.1,1,5)\n",
    "                }\n",
    "\n",
    "grid = GridSearchCV(estimator=SVR(),param_grid=param_grid,verbose=2,scoring='neg_mean_absolute_error')\n",
    "grid.fit(X, df['kpdate'])\n",
    "print(\"Best parameter values:\", grid.best_params_)\n",
    "print(\"CV Score with best parameter values:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Best parameter values:\", grid.best_params_)\n",
    "print(\"CV Score with best parameter values:\", grid.best_score_)\n",
    "# Best parameter values: {'C': 30.0, 'epsilon': 0.1, 'gamma': 10.0, 'kernel': 'rbf'}\n",
    "# CV Score with best parameter values: -0.8477903605840826"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "                  'kernel':['rbf'],\n",
    "                  'C':np.linspace(100,1000,5),\n",
    "                  'gamma':np.linspace(25,35,5),\n",
    "                }\n",
    "\n",
    "grid = GridSearchCV(estimator=SVR(),param_grid=param_grid,verbose=2,scoring='neg_mean_absolute_error')\n",
    "grid.fit(X, df['kpdate'])\n",
    "print(\"Best parameter values:\", grid.best_params_)\n",
    "print(\"CV Score with best parameter values:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['law'], df['kpdate'], test_size=0.1, random_state=0)\n",
    "tf_vectorizer=TfidfVectorizer(ngram_range=(1,1),encoding='utf-8',sublinear_tf=True) #min_df=0 , #binary maybe sublinear_tf=true\n",
    "X_train=tf_vectorizer.fit_transform(X_train)\n",
    "X_test=tf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clf = KernelRidge(kernel='rbf',alpha=0.041,gamma = 0.015) #0.015\n",
    "clf.fit(X_train,y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "mean = mean_absolute_error(y_test, predicted)\n",
    "print(mean) #1.5281275405560835\n",
    "calculate_special_mae(predicted) #1.3996126603349304"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"berts/legal-bert-base-uncased\") #dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
    "model = AutoModel.from_pretrained(\"berts/legal-bert-base-uncased\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "#spacy_nlp = spacy.load('C:\\\\Users\\\\sichi\\\\PycharmProjects\\\\NLPproiect\\\\venv\\\\Lib\\\\site-packages\\\\en_core_web_sm\\\\en_core_web_sm-2.3.1')\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df = pd.read_csv('input.csv', dtype={col:str for col in df.columns})\n",
    "\n",
    "def clean_numbers(row,col):\n",
    "    year = row['kpdate'].split('-')[0]\n",
    "    article = row['appno']\n",
    "    if article is None:\n",
    "        article = row['extractedappno']\n",
    "    row[col] = row[col].replace(year,'YEAR')\n",
    "    for x in article.split(';'):\n",
    "        row[col] = row[col].replace(x,'APPNUMBER')\n",
    "    if(re.search(\"^\\s*[0-9]+\\.\",str(row['law']))):\n",
    "        app_n = str(row['law']).split('.')[0] + '.'\n",
    "        row[col] = row[col].replace(app_n,'NUMBER')\n",
    "    return row[col]\n",
    "\n",
    "def sliding_window(input_ids):\n",
    "    #this bert only support a max length of 512 words\n",
    "    to_return = []\n",
    "    for slice_list in torch.split(input_ids,512,dim = 1):\n",
    "        tens = model(slice_list)[0][:,0,:].detach().numpy()\n",
    "        to_return.append(tens)\n",
    "    print(len(to_return))\n",
    "    return numpy.concatenate(to_return) #, dim=1\n",
    "\n",
    "# df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "# for index, row in df.iterrows():\n",
    "#     df.at[index, 'text'] = clean_numbers(row,'text')\n",
    "#df['text'] = df['text'].apply(lambda x: spacy_nlp.tokenizer(x))\n",
    "#df['text'] = df['text'].apply(lambda sentence: ' '.join([token.text for token in sentence if token.is_stop == False and token.is_punct == False and token.text not in ners]))\n",
    "\n",
    "#df['text'] = df['text'].apply(lambda x: tokenizer.encode_plus(x,return_tensors = 'pt', padding=True,pad_to_multiple_of=128))\n",
    "#df['text'] = df['text'].apply(lambda x: sliding_window(x['input_ids'])) # **x\n",
    "\n",
    "\n",
    "df['law'] = df['law'].apply(lambda x: str(x).lower())\n",
    "for index, row in df.iterrows():\n",
    "    df.at[index, 'law'] = clean_numbers(row,'law')\n",
    "df['law'] = df['law'].apply(lambda x: tokenizer.encode_plus(x,return_tensors = 'pt', padding=True,pad_to_multiple_of=128))\n",
    "df['law'] = df['law'].apply(lambda x: sliding_window(x['input_ids'])) # **x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "#spacy_nlp = spacy.load('C:\\\\Users\\\\sichi\\\\PycharmProjects\\\\NLPproiect\\\\venv\\\\Lib\\\\site-packages\\\\en_core_web_sm\\\\en_core_web_sm-2.3.1')\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df = pd.read_csv('input.csv', dtype={col:str for col in df.columns})\n",
    "\n",
    "def clean_numbers(row,col):\n",
    "    year = row['kpdate'].split('-')[0]\n",
    "    article = row['appno']\n",
    "    if article is None:\n",
    "        article = row['extractedappno']\n",
    "    row[col] = row[col].replace(year,'YEAR')\n",
    "    for x in article.split(';'):\n",
    "        row[col] = row[col].replace(x,'APPNUMBER')\n",
    "    if(re.search(\"^\\s*[0-9]+\\.\",str(row['law']))):\n",
    "        app_n = str(row['law']).split('.')[0] + '.'\n",
    "        row[col] = row[col].replace(app_n,'NUMBER')\n",
    "    return row[col]\n",
    "\n",
    "def sliding_window(input_ids):\n",
    "    #this bert only support a max length of 512 words\n",
    "    to_return = []\n",
    "    for slice_list in torch.split(input_ids,512,dim = 1):\n",
    "        tens = model(slice_list)[0][:,0,:].detach().numpy()\n",
    "        to_return.append(tens)\n",
    "    print(len(to_return))\n",
    "    return numpy.concatenate(to_return) #, dim=1\n",
    "\n",
    "# df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "# for index, row in df.iterrows():\n",
    "#     df.at[index, 'text'] = clean_numbers(row,'text')\n",
    "#df['text'] = df['text'].apply(lambda x: spacy_nlp.tokenizer(x))\n",
    "#df['text'] = df['text'].apply(lambda sentence: ' '.join([token.text for token in sentence if token.is_stop == False and token.is_punct == False and token.text not in ners]))\n",
    "\n",
    "#df['text'] = df['text'].apply(lambda x: tokenizer.encode_plus(x,return_tensors = 'pt', padding=True,pad_to_multiple_of=128))\n",
    "#df['text'] = df['text'].apply(lambda x: sliding_window(x['input_ids'])) # **x\n",
    "\n",
    "\n",
    "df['law'] = df['law'].apply(lambda x: str(x).lower())\n",
    "for index, row in df.iterrows():\n",
    "    df.at[index, 'law'] = clean_numbers(row,'law')\n",
    "df['law'] = df['law'].apply(lambda x: tokenizer.encode_plus(x,return_tensors = 'pt', padding=True,pad_to_multiple_of=128))\n",
    "df['law'] = df['law'].apply(lambda x: sliding_window(x['input_ids'])) # **x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "def sliding_window(input_ids):\n",
    "    #this bert only support a max length of 512 words\n",
    "    to_return = []\n",
    "    for slice_list in torch.split(input_ids,512,dim = 1):\n",
    "        tens = model(slice_list)[0][:,0,:].detach().numpy() #last_hidden_state[0]\n",
    "        to_return.append(tens)\n",
    "    print(len(to_return))\n",
    "    #return torch.cat(to_return) #, dim=1\n",
    "    return numpy.concatenate(tens)\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: sliding_window(x['input_ids'])) # **x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sliding_window(input_ids):\n",
    "    #this bert only support a max length of 512 words\n",
    "    to_return = []\n",
    "    for i in range(0,int(len(input_ids)/512),1):\n",
    "        slice_list = input_ids[512 * i,512*(i+1)]\n",
    "        to_return.append(model(slice_list))\n",
    "    return to_return\n",
    "\n",
    "df['law'] = df['law'].apply(lambda x: sliding_window(x['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
